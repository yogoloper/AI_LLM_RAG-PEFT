"""
ê°„ì†Œí™”ëœ RAG ì‹œìŠ¤í…œ - PDF ì—…ë¡œë“œ ì§€ì›
"""

import streamlit as st
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import faiss
import numpy as np
from datasets import load_dataset
from typing import List, Dict, Tuple
from api_client import VLLMAPIClient
import PyPDF2
import pdfplumber
import io
import os
from datetime import datetime

class SimpleRAGSystem:
    """ê°„ì†Œí™”ëœ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, api_client: VLLMAPIClient):
        self.api_client = api_client
        self.chunks = []
        self.chunk_metadata = []
        self.vector_index = None
        self.embedder = None
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.initialized = False
        
        # PDF ê´€ë ¨ ìƒíƒœ
        self.pdf_documents = []  # ì—…ë¡œë“œëœ PDF ë¬¸ì„œë“¤
        self.pdf_chunks = []     # PDFì—ì„œ ì¶”ì¶œí•œ ì²­í¬ë“¤
        self.pdf_metadata = []   # PDF ì²­í¬ ë©”íƒ€ë°ì´í„°
    
    @st.cache_resource
    def load_embedding_model(_self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ìºì‹œ)"""
        return SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    
    def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        if self.initialized:
            return
        
        try:
            # ëŒ€ì•ˆ ë°ì´í„°ì…‹ ì‚¬ìš© (ë” ì ì€ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸)
            print("ğŸ“¦ ëŒ€ì•ˆ ë°ì´í„°ì…‹ ì‚¬ìš©...")
            knowledge_data = [
                {
                    'index': 0,
                    'user_message': 'How do I reset my password and update my profile?',
                    'context': 'To reset your password, go to the login page and click "Forgot Password". Enter your email to receive reset instructions. To update your profile, log into your account and navigate to Settings or Profile section where you can edit your information.',
                    'response': 'You can reset your password by clicking "Forgot Password" on the login page and following the email instructions. To update your profile, access the Settings section after logging in.'
                },
                {
                    'index': 1,
                    'user_message': 'What is Argilla and how does it work?',
                    'context': 'Argilla is a comprehensive Software as a Service (SaaS) solution for data labeling and curation. The service is designed to meet the needs of businesses seeking a reliable, secure, and user-friendly platform for data management. Argilla provides tools for annotation, quality control, and data workflow management.',
                    'response': 'Argilla is a SaaS platform for data labeling and curation that helps businesses manage their data annotation workflows efficiently.'
                },
                {
                    'index': 2,
                    'user_message': 'How can I contact customer support?',
                    'context': 'Customer support can be reached through multiple channels: email support is available 24/7, live chat during business hours (9 AM - 6 PM), and phone support for urgent issues. You can also submit tickets through the help desk system.',
                    'response': 'You can contact customer support via email (24/7), live chat (business hours), phone for urgent issues, or through our help desk ticketing system.'
                },
                {
                    'index': 3,
                    'user_message': 'What are the backup and recovery procedures?',
                    'context': 'Argilla Cloud provides comprehensive backup and recovery protocol with daily backups. The service has a recovery point objective (RPO) of 24 hours and recovery time objective (RTO) designed to minimize data loss and ensure swift recovery in case of disruption.',
                    'response': 'We perform daily backups with 24-hour RPO and have established RTO procedures to ensure quick data recovery in case of any service disruption.'
                },
                {
                    'index': 4,
                    'user_message': 'How do I manage user access and permissions?',
                    'context': 'User access management is handled through the administrator panel where you can invite team members, assign roles, and set permissions. The client administrator has full control over their teams access and can manage workspace settings efficiently.',
                    'response': 'Access the administrator panel to invite users, assign roles, and manage permissions. Client administrators have full control over team access and workspace management.'
                }
            ]
            print(f"âœ… {len(knowledge_data)}ê°œ ê³ í’ˆì§ˆ ë°ì´í„° ì¤€ë¹„ë¨")
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            raise e
        
        # ë°ì´í„° í™•ì¸
        if not knowledge_data:
            raise ValueError("ì§€ì‹ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print("ğŸ“¦ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.embedder = self.load_embedding_model()
        
        # Context ì²­í‚¹
        print("ğŸ“ í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...")
        self._chunk_contexts(knowledge_data)
        
        # ë°ì´í„° í™•ì¸
        if not self.chunks:
            raise ValueError("ì²­í‚¹ í›„ì—ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
        print(f"ğŸ“Š ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(self.chunks)}")
        
        # ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•
        print("ğŸ” ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        self._build_vector_index()
        
        # TF-IDF ì¸ë±ìŠ¤ êµ¬ì¶•
        print("ğŸ“ˆ TF-IDF ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        self._build_tfidf_index()
        
        print("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        self.initialized = True
    
    def _chunk_contexts(self, knowledge_data: List[Dict]):
        """Contextë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunk_size = 300
        self.chunks = []
        self.chunk_metadata = []
        
        for item in knowledge_data:
            context = item['context']
            # ê°„ë‹¨í•œ ì²­í‚¹ (ë¬¸ì¥ ë‹¨ìœ„)
            sentences = context.split('. ')
            
            current_chunk = ""
            for sentence in sentences:
                if len(current_chunk + sentence) < chunk_size:
                    current_chunk += sentence + ". "
                else:
                    if current_chunk:
                        self.chunks.append(current_chunk.strip())
                        self.chunk_metadata.append({
                            'source_index': item['index'],
                            'user_message': item['user_message'],
                            'response': item['response']
                        })
                    current_chunk = sentence + ". "
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
            if current_chunk:
                self.chunks.append(current_chunk.strip())
                self.chunk_metadata.append({
                    'source_index': item['index'],
                    'user_message': item['user_message'],
                    'response': item['response']
                })
    
    def _build_vector_index(self):
        """ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•"""
        embeddings = self.embedder.encode(self.chunks)
        
        # FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
        dimension = embeddings.shape[1]
        self.vector_index = faiss.IndexFlatL2(dimension)
        self.vector_index.add(embeddings.astype('float32'))
    
    def _build_tfidf_index(self):
        """TF-IDF ì¸ë±ìŠ¤ êµ¬ì¶•"""
        self.tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.chunks)
    
    def semantic_search(self, query: str, k: int = 3) -> List[Tuple[str, Dict, float]]:
        """ì˜ë¯¸ì  ê²€ìƒ‰"""
        query_embedding = self.embedder.encode([query])
        distances, indices = self.vector_index.search(query_embedding.astype('float32'), k)
        
        results = []
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            if idx < len(self.chunks):
                similarity = 1 / (1 + distance)  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                results.append((
                    self.chunks[idx],
                    self.chunk_metadata[idx],
                    similarity
                ))
        
        return results
    
    def keyword_search(self, query: str, k: int = 3) -> List[Tuple[str, Dict, float]]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰"""
        query_vector = self.tfidf_vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
        
        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤
        top_indices = similarities.argsort()[-k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0:  # ìœ ì‚¬ë„ê°€ 0ë³´ë‹¤ í° ê²½ìš°ë§Œ
                results.append((
                    self.chunks[idx],
                    self.chunk_metadata[idx],
                    similarities[idx]
                ))
        
        return results
    
    def hybrid_search(self, query: str, k: int = 3) -> List[Tuple[str, Dict, float]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        # ì˜ë¯¸ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°í•©
        semantic_results = self.semantic_search(query, k)
        keyword_results = self.keyword_search(query, k)
        
        # ê²°ê³¼ í†µí•© (ê°„ë‹¨í•œ ë°©ì‹)
        all_results = {}
        
        # ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ (ê°€ì¤‘ì¹˜ 0.6)
        for chunk, metadata, score in semantic_results:
            key = chunk[:50]  # ì²­í¬ì˜ ì²˜ìŒ 50ìë¥¼ í‚¤ë¡œ ì‚¬ìš©
            all_results[key] = (chunk, metadata, score * 0.6)
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ (ê°€ì¤‘ì¹˜ 0.4)
        for chunk, metadata, score in keyword_results:
            key = chunk[:50]
            if key in all_results:
                # ì´ë¯¸ ìˆëŠ” ê²½ìš° ì ìˆ˜ í•©ì‚°
                existing_chunk, existing_metadata, existing_score = all_results[key]
                all_results[key] = (existing_chunk, existing_metadata, existing_score + score * 0.4)
            else:
                all_results[key] = (chunk, metadata, score * 0.4)
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        sorted_results = sorted(all_results.values(), key=lambda x: x[2], reverse=True)
        return sorted_results[:k]
    
    def generate_answer(self, query: str, search_method: str = "hybrid") -> Tuple[str, List[Dict]]:
        """ë‹µë³€ ìƒì„±"""
        if not self.initialized:
            return "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", []
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        if search_method == "semantic":
            search_results = self.semantic_search(query)
        elif search_method == "keyword":
            search_results = self.keyword_search(query)
        else:
            search_results = self.hybrid_search(query)
        
        if not search_results:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        source_docs = []
        
        for chunk, metadata, score in search_results:
            context_parts.append(f"- {chunk}")
            source_docs.append({
                'chunk': chunk,
                'score': score,
                'metadata': metadata
            })
        
        context = "\n".join(context_parts)
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì˜¤ë²„í”Œë¡œìš° ë°©ì§€)
        max_context_length = 800  # ì¶©ë¶„í•œ ì—¬ìœ ë¥¼ ë‘ê³  ì„¤ì •
        if len(context) > max_context_length:
            context = context[:max_context_length] + "..."
            print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ì„œ {max_context_length}ìë¡œ ì˜ë ¸ìŠµë‹ˆë‹¤.")
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ê°„ì†Œí™”)
        prompt = f"""ì°¸ê³  ì •ë³´: {context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""
        
        # API í˜¸ì¶œ
        answer = self.api_client.simple_chat(prompt)
        
        return answer, source_docs
    
    def extract_text_from_pdf(self, pdf_file) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # pdfplumberë¥¼ ìš°ì„  ì‚¬ìš© (ë” ì •í™•í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ)
            with pdfplumber.open(pdf_file) as pdf:
                text = ""
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                
                if text.strip():
                    return text
            
            # pdfplumber ì‹¤íŒ¨ì‹œ PyPDF2 ì‚¬ìš©
            pdf_file.seek(0)  # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            
            return text
            
        except Exception as e:
            st.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return ""
    
    def add_pdf_document(self, pdf_file, filename: str) -> bool:
        """PDF ë¬¸ì„œë¥¼ RAG ì‹œìŠ¤í…œì— ì¶”ê°€"""
        try:
            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            pdf_text = self.extract_text_from_pdf(pdf_file)
            
            if not pdf_text.strip():
                st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # PDF ë¬¸ì„œ ì •ë³´ ì €ì¥
            pdf_doc = {
                'filename': filename,
                'upload_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'text': pdf_text,
                'chunk_count': 0
            }
            
            # PDF í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹
            pdf_chunks = self._chunk_pdf_text(pdf_text, filename)
            
            if not pdf_chunks:
                st.error("PDF í…ìŠ¤íŠ¸ ì²­í‚¹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return False
            
            pdf_doc['chunk_count'] = len(pdf_chunks)
            self.pdf_documents.append(pdf_doc)
            
            # ê¸°ì¡´ ì²­í¬ì™€ í†µí•©
            self.chunks.extend([chunk['text'] for chunk in pdf_chunks])
            self.chunk_metadata.extend([chunk['metadata'] for chunk in pdf_chunks])
            
            # ë²¡í„° ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
            self._rebuild_indices()
            
            st.success(f"âœ… PDF '{filename}' ì¶”ê°€ ì™„ë£Œ! ({len(pdf_chunks)}ê°œ ì²­í¬)")
            return True
            
        except Exception as e:
            st.error(f"PDF ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _chunk_pdf_text(self, text: str, filename: str) -> List[Dict]:
        """PDF í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunk_size = 500  # PDFëŠ” ì¢€ ë” í° ì²­í¬ ì‚¬ìš©
        overlap = 50      # ì²­í¬ ê°„ ì˜¤ë²„ë©
        
        chunks = []
        sentences = text.split('. ')
        
        current_chunk = ""
        for i, sentence in enumerate(sentences):
            if len(current_chunk + sentence) < chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk.strip():
                    chunks.append({
                        'text': current_chunk.strip(),
                        'metadata': {
                            'source_type': 'pdf',
                            'filename': filename,
                            'chunk_index': len(chunks),
                            'upload_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                    })
                
                # ì˜¤ë²„ë©ì„ ìœ„í•´ ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ í¬í•¨
                if overlap > 0 and len(sentences) > i + 1:
                    overlap_start = max(0, i - overlap//10)  # ëŒ€ëµì ì¸ ì˜¤ë²„ë©
                    current_chunk = '. '.join(sentences[overlap_start:i+1]) + ". "
                else:
                    current_chunk = sentence + ". "
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk.strip():
            chunks.append({
                'text': current_chunk.strip(),
                'metadata': {
                    'source_type': 'pdf',
                    'filename': filename,
                    'chunk_index': len(chunks),
                    'upload_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
            })
        
        return chunks
    
    def _rebuild_indices(self):
        """ë²¡í„° ë° TF-IDF ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
        if not self.chunks or not self.embedder:
            return
        
        # ë²¡í„° ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
        print("ğŸ”„ ë²¡í„° ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì¤‘...")
        embeddings = self.embedder.encode(self.chunks)
        dimension = embeddings.shape[1]
        self.vector_index = faiss.IndexFlatL2(dimension)
        self.vector_index.add(embeddings.astype('float32'))
        
        # TF-IDF ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
        print("ğŸ”„ TF-IDF ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì¤‘...")
        self.tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.chunks)
        
        print(f"âœ… ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì™„ë£Œ! (ì´ {len(self.chunks)}ê°œ ì²­í¬)")
    
    def get_pdf_summary(self) -> Dict:
        """ì—…ë¡œë“œëœ PDF ë¬¸ì„œ ìš”ì•½ ì •ë³´"""
        return {
            'total_pdfs': len(self.pdf_documents),
            'total_pdf_chunks': sum([doc['chunk_count'] for doc in self.pdf_documents]),
            'documents': self.pdf_documents
        }
    
    def remove_pdf_document(self, filename: str) -> bool:
        """íŠ¹ì • PDF ë¬¸ì„œ ì œê±°"""
        try:
            # PDF ë¬¸ì„œ ì°¾ê¸°
            pdf_to_remove = None
            for doc in self.pdf_documents:
                if doc['filename'] == filename:
                    pdf_to_remove = doc
                    break
            
            if not pdf_to_remove:
                st.warning(f"PDF '{filename}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # í•´ë‹¹ PDFì˜ ì²­í¬ë“¤ ì œê±°
            new_chunks = []
            new_metadata = []
            
            for chunk, metadata in zip(self.chunks, self.chunk_metadata):
                if metadata.get('source_type') != 'pdf' or metadata.get('filename') != filename:
                    new_chunks.append(chunk)
                    new_metadata.append(metadata)
            
            self.chunks = new_chunks
            self.chunk_metadata = new_metadata
            
            # PDF ë¬¸ì„œ ëª©ë¡ì—ì„œ ì œê±°
            self.pdf_documents = [doc for doc in self.pdf_documents if doc['filename'] != filename]
            
            # ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
            self._rebuild_indices()
            
            st.success(f"âœ… PDF '{filename}' ì œê±° ì™„ë£Œ!")
            return True
            
        except Exception as e:
            st.error(f"PDF ë¬¸ì„œ ì œê±° ì‹¤íŒ¨: {str(e)}")
            return False