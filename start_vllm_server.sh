#!/bin/bash

# A100 ì„œë²„ìš© VLLM ì„œë²„ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
# model_tuning.ipynbì—ì„œ í›ˆë ¨í•œ íŠœë‹ëœ ëª¨ë¸ ì‚¬ìš©

echo "ğŸš€ A100 ì„œë²„ì—ì„œ VLLM ì„œë²„ ì‹œì‘..."

# ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸ ë° ìë™ í™œì„±í™”
if [ -z "$VIRTUAL_ENV" ]; then
    if [ -d "./venv" ]; then
        echo "ğŸ”„ ê°€ìƒí™˜ê²½ í™œì„±í™” ì¤‘..."
        source ./venv/bin/activate
        echo "âœ… ê°€ìƒí™˜ê²½ í™œì„±í™”ë¨: $VIRTUAL_ENV"
    else
        echo "âŒ ê°€ìƒí™˜ê²½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        echo "ğŸ’¡ ë¨¼ì € setup.shë¥¼ ì‹¤í–‰í•˜ì—¬ í™˜ê²½ì„ ì„¤ì •í•˜ì„¸ìš”:"
        echo "   bash setup.sh"
        exit 1
    fi
else
    echo "âœ… ê°€ìƒí™˜ê²½ ì´ë¯¸ í™œì„±í™”ë¨: $VIRTUAL_ENV"
fi

# vLLM ì„¤ì¹˜ í™•ì¸
if ! python -c "import vllm" >/dev/null 2>&1; then
    echo "âŒ vLLMì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "ğŸ’¡ ê°€ìƒí™˜ê²½ì—ì„œ vLLMì„ ì„¤ì¹˜í•˜ì„¸ìš”:"
    echo "   pip install vllm"
    exit 1
fi

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export CUDA_VISIBLE_DEVICES=0
export VLLM_ATTENTION_BACKEND=FLASH_ATTN

# í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ í•¨ìˆ˜
cleanup() {
    echo "\nğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘..."
    pkill -f "vllm.entrypoints.openai.api_server"
    exit 0
}

# ì‹ í˜¸ ì²˜ë¦¬
trap cleanup SIGINT SIGTERM

# ê¸°ì¡´ VLLM í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
echo "ğŸ§¹ ê¸°ì¡´ VLLM í”„ë¡œì„¸ìŠ¤ ì •ë¦¬..."
pkill -f "vllm.entrypoints.openai.api_server" 2>/dev/null || true
sleep 2

# íŠœë‹ëœ ëª¨ë¸ ê²½ë¡œ í™•ì¸ (ìˆœì„œëŒ€ë¡œ ì²´í¬)
MODEL_PATHS=("./final-tuned-model" "./sft-final-model" "./sft-model-advanced" "./dpo-alternative-advanced")
MODEL_PATH=""

for path in "${MODEL_PATHS[@]}"; do
    if [ -d "$path" ] && [ -f "$path/adapter_config.json" ]; then
        MODEL_PATH="$path"
        echo "âœ… íŠœë‹ëœ LoRA ëª¨ë¸ ë°œê²¬: $MODEL_PATH"
        break
    fi
done

if [ -z "$MODEL_PATH" ]; then
    echo "âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    echo "ë‹¤ìŒ ê²½ë¡œë“¤ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤:"
    for path in "${MODEL_PATHS[@]}"; do
        echo "  - $path"
    done
    echo "ğŸ’¡ model_tuning.ipynbë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”."
    exit 1
fi

echo "ğŸ“¦ ì‚¬ìš©í•  ëª¨ë¸: $MODEL_PATH"

# GPU ìƒíƒœ í™•ì¸
echo "ğŸ–¥ï¸  GPU ìƒíƒœ í™•ì¸:"
nvidia-smi --query-gpu=name,memory.total,memory.used,memory.free --format=csv,noheader,nounits

echo "\nğŸ”§ VLLM ì„œë²„ ì„¤ì • (ë©”ëª¨ë¦¬ ìµœì í™”):"
echo "   - ë² ì´ìŠ¤ ëª¨ë¸: meta-llama/Llama-2-7b-chat-hf"
echo "   - LoRA ì–´ëŒ‘í„°: $MODEL_PATH"
echo "   - í¬íŠ¸: 8000"
echo "   - GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : 70% (0.7)"
echo "   - ìµœëŒ€ í† í°: 2048"
echo "   - ìµœëŒ€ ë™ì‹œ ì‹œí€€ìŠ¤: 32ê°œ"
echo "   - CORS: ê¸°ë³¸ ì„¤ì •"

# í¬íŠ¸ ì‚¬ìš© í™•ì¸
if command -v lsof >/dev/null 2>&1 && lsof -Pi :8000 -sTCP:LISTEN -t >/dev/null 2>&1; then
    echo "âš ï¸  í¬íŠ¸ 8000ì´ ì´ë¯¸ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."
    lsof -ti:8000 | xargs kill -9 2>/dev/null || true
    sleep 2
fi

echo "\nğŸš€ VLLM ì„œë²„ ì‹œì‘ ì¤‘..."
echo "ğŸ“¡ ì ‘ê·¼URL: http://localhost:8000"
echo "ğŸ”— Health Check: http://localhost:8000/v1/models"
echo "\n[Ctrl+Cë¡œ ì¢…ë£Œ]"

# VLLM ì„œë²„ ì‹œì‘ (A100 ìµœì í™” ì„¤ì • + LoRA ì–´ëŒ‘í„°)
python -m vllm.entrypoints.openai.api_server \
    --model "meta-llama/Llama-2-7b-chat-hf" \
    --enable-lora \
    --lora-modules tuned-model="$MODEL_PATH" \
    --host 0.0.0.0 \
    --port 8000 \
    --served-model-name "tuned-model" \
    --max-model-len 2048 \
    --tensor-parallel-size 1 \
    --dtype auto \
    --trust-remote-code \
    --disable-log-stats \
    --gpu-memory-utilization 0.7 \
    --max-num-seqs 32 \
    --swap-space 2 \
    --enable-chunked-prefill \
    --max-num-batched-tokens 2048

echo "\nğŸ VLLM ì„œë²„ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."